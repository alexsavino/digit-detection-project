{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import scipy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from scipy.signal import convolve2d as conv2\n",
    "from skimage import restoration\n",
    "\n",
    "\n",
    "\n",
    "from intermediate_code.data_preprocessing import raw_data_to_tensor_dataset\n",
    "from intermediate_code.CNN import CNN\n",
    "from intermediate_code.model_training_evmoaluator import CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- DEFINING EITHER CNN / VGG16 TRAINING + EVALUATION PIPELINE FOR A GIVEN COMBINATION OF HYPERPARAMETERS ---\n",
    "def train_and_evaluate_model(base_dataset_path, model_name, loss_function, learning_rate, batch_size):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    best_validation_error = float('inf')\n",
    "    best_model_state = None # To store the state of the best model\n",
    "\n",
    "    # Properly Loading in All Data ..............................................................\n",
    "    training_dataset = None\n",
    "    testing_dataset = None\n",
    "    for child in os.listdir(base_dataset_path):\n",
    "        child_path = os.path.join(base_dataset_path, child)\n",
    "        if child.startswith('train'):\n",
    "            if os.path.isfile(child_path):\n",
    "                if child.endswith('.mat'):\n",
    "                    training_dataset = raw_data_to_tensor_dataset(child_path, batch_size)\n",
    "            # elif os.path.isdir(child_path)\n",
    "        elif child.startswith('test'):\n",
    "            if os.path.isfile(child_path):\n",
    "                if child.endswith('.mat'):\n",
    "                    testing_dataset = raw_data_to_tensor_dataset(child_path, batch_size)\n",
    "            # elif os.path.isdir(child_path)\n",
    "            \n",
    "    if training_dataset is None or testing_dataset is None:\n",
    "        raise ValueError(\"Training or testing dataset not found.\")\n",
    "\n",
    "    # Defining the (constant) loss function......................................................\n",
    "    if loss_function=='cross-entropy':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss_function=='mean-squared':\n",
    "        criterion = nn.MSELoss()\n",
    "    elif loss_function=='multi-margin':\n",
    "        criterion = nn.MultiMarginLoss()\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid loss function: {loss_function}\")\n",
    "    \n",
    "    # Model Initialization ......................................................................\n",
    "    if model_name == 'vgg16':\n",
    "        model = models.vgg16(weights='IMAGENET1K_V1')                                  # downloading a vgg16 with pre-trained weights\n",
    "        model.classifier[6] = nn.Linear(4096, 11)  # Adjust for 11 classes (SVHN)\n",
    "    elif model_name == 'original':\n",
    "        model = CNN()                                                                  # reseting the model + optimizer each fold\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model name: {model_name}\")\n",
    "    \n",
    "    best_model = model # Keep track of the model instance.\n",
    "\n",
    "    # Training + Determining Training + Validation Error ........................................\n",
    "    training_error = 0\n",
    "    validation_error = 0\n",
    "\n",
    "    k = 3\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(training_dataset)):\n",
    "        print(f\"Training fold {fold + 1}/{k}...\")\n",
    "\n",
    "        # Separating Full Training Dataset into Training + Validation ...........................\n",
    "        training_subset = Subset(training_dataset, train_idx)                          # creating subsets + official dataloaders of the full training_dataset for cross validation\n",
    "        val_subset = Subset(training_dataset, val_idx)\n",
    "        \n",
    "        training_loader = DataLoader(training_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "        # (CNN) Model Initialization ............................................................\n",
    "        if model_name == 'original':\n",
    "            model = CNN()                                                              # reseting the model + optimizer each fold.  Create a new instance for each fold.\n",
    "        \n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)  # (optimizers adjust model parmeters to minimize the loss function!! it doesn't actually do the backprop, but updates the model using the gradients calculated during that)\n",
    "\n",
    "\n",
    "        # Training Loop .........................................................................\n",
    "        model.train()                                                                  # setting the model to training mode!\n",
    "        for epoch in range(1):                                                         # at a high level... for each testing + validation set combo, you train x epochs amount of times, and then you evaulate the validation accuracy\n",
    "            for inputs, labels in training_loader:\n",
    "                optimizer.zero_grad()                                                  # zero-ing out / restarting the gradients ... (BUT THE MODEL WEIGHTS ARE NEVER ZERO-ED!)\n",
    "                outputs = model(inputs)                                                # running all the input data through to get some predictions\n",
    "                loss = criterion(outputs, labels)                                      # calculating loss wrt the loss function, comparing the predictions with the actuals\n",
    "                loss.backward()                                                        # performs backprop\n",
    "                optimizer.step()                                                       # updates the model parameters using what was found during backprop\n",
    "\n",
    "\n",
    "        # Training Evaluation ....................................................................\n",
    "        model.eval()                                                                   # setting the model to evaluation mode!\n",
    "        correct_train, total_train = 0, 0\n",
    "        with torch.no_grad():                                                          # turns off gradient checking for memory-saving purposes\n",
    "            for inputs, labels in training_loader:                                     # loops through all the batches\n",
    "                outputs = model(inputs)                                                # gets the raw predictions back\n",
    "                _, predicted = torch.max(outputs, 1)                                   # gets the class with the highest score\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "        training_error_fold = (1 - (correct_train/total_train))\n",
    "\n",
    "        # Validation Evaluation ..................................................................\n",
    "        correct_val, total_val = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += labels.size(0)\n",
    "                correct_val += (predicted == labels).sum().item()\n",
    "        validation_error_fold = (1 - (correct_val/total_val))\n",
    "\n",
    "\n",
    "        # Saving the Best Model!\n",
    "        if validation_error_fold < best_validation_error:\n",
    "            best_validation_error = validation_error_fold\n",
    "            best_model_state = model.state_dict() # Save the model's weights\n",
    "        \n",
    "        print(f\"Fold {fold + 1}: Training Error = {training_error_fold*100:.2f}%, Validation Error = {validation_error_fold*100:.2f}%\")\n",
    "        training_error += training_error_fold\n",
    "        validation_error += validation_error_fold\n",
    "\n",
    "\n",
    "    training_error /= k\n",
    "    validation_error /= k\n",
    "\n",
    "\n",
    "    # Testing Evaluation ........................................................................\n",
    "    testing_loader = DataLoader(testing_dataset, batch_size=batch_size, shuffle=False) # Use the test dataset!\n",
    "\n",
    "    testing_error = 0\n",
    "    correct_test, total_test = 0, 0\n",
    "\n",
    "    model.eval() # Set to eval mode before testing.\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testing_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "    \n",
    "    testing_error = (1 - (correct_test/total_test))\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_WCT = end_time - start_time\n",
    "\n",
    "    # Saving the Overall Best Model!\n",
    "    if best_model_state is not None:\n",
    "        # best_model.load_state_dict(best_model_state) # Remove this line\n",
    "        torch.save(best_model_state, f\"current-models/original/{loss_function}-{str(batch_size)}-{str(learning_rate)}.pth\")\n",
    "    else:\n",
    "        print(\"Warning: No best model state found. Saving the last model.\")\n",
    "        torch.save(model.state_dict(), f\"current-models/original/{loss_function}-{str(batch_size)}-{str(learning_rate)}.pth\") # Save the last model's weights.\n",
    "\n",
    "    return training_error*100, validation_error*100, testing_error*100, total_WCT\n",
    "\n",
    "\n",
    "# --- DEFINING EASY-USE HYPERPARAMETER GRIDSEARCHING FUNCTION ---\n",
    "def grid_search_hyperparameters(base_dataset_path, model_name, loss_function_list, learning_rate_list, batch_size_list):\n",
    "\n",
    "    # {key: value} == {(loss_function, learning_rate, batch_size): \n",
    "    #                         (training_error, validation_error, testing_error))}\n",
    "    hyperparameter_performance_results = {}\n",
    "\n",
    "    i = 0\n",
    "    for loss_function in loss_function_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "            for batch_size in batch_size_list:\n",
    "                print(\"iteration: \", i)\n",
    "                hyperparameter_performance_results[(loss_function, learning_rate, batch_size)] = train_and_evaluate_model(base_dataset_path, model_name, loss_function, learning_rate, batch_size)\n",
    "                i += 1\n",
    "\n",
    "    return hyperparameter_performance_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  0\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 85.58%, Validation Error = 85.49%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.92%, Validation Error = 81.44%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 84.84%, Validation Error = 84.53%\n",
      "iteration:  1\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 93.62%, Validation Error = 93.70%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 91.97%, Validation Error = 91.71%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 95.62%, Validation Error = 95.49%\n",
      "iteration:  2\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 93.15%, Validation Error = 93.03%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 93.65%, Validation Error = 93.62%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 88.40%, Validation Error = 88.39%\n",
      "iteration:  3\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 85.58%, Validation Error = 85.49%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 99.93%, Validation Error = 99.92%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 93.65%, Validation Error = 93.61%\n",
      "iteration:  4\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.91%, Validation Error = 81.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  5\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.91%, Validation Error = 81.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  6\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.91%, Validation Error = 81.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  7\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.91%, Validation Error = 81.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  8\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.12%, Validation Error = 80.92%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 75.41%, Validation Error = 75.56%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 73.26%, Validation Error = 72.96%\n",
      "iteration:  9\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 38.37%, Validation Error = 39.01%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 36.06%, Validation Error = 36.62%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 41.53%, Validation Error = 41.49%\n",
      "iteration:  10\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 22.74%, Validation Error = 23.00%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 18.45%, Validation Error = 19.22%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 21.73%, Validation Error = 22.38%\n",
      "iteration:  11\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 23.34%, Validation Error = 23.87%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 22.77%, Validation Error = 23.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 22.41%, Validation Error = 22.83%\n",
      "iteration:  12\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 88.41%, Validation Error = 88.38%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 88.39%, Validation Error = 88.43%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 85.49%, Validation Error = 85.66%\n",
      "iteration:  13\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.91%, Validation Error = 81.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  14\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 88.39%, Validation Error = 88.43%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  15\n",
      "Training fold 1/3...\n",
      "Fold 1: Training Error = 81.14%, Validation Error = 80.96%\n",
      "Training fold 2/3...\n",
      "Fold 2: Training Error = 80.91%, Validation Error = 81.42%\n",
      "Training fold 3/3...\n",
      "Fold 3: Training Error = 81.19%, Validation Error = 80.86%\n",
      "iteration:  16\n",
      "Training fold 1/3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexandrasavino/miniconda3/envs/cv_proj_mac/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 11])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (11) must match the size of tensor b (32) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m learning_rate_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m batch_size_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m]\n\u001b[0;32m---> 12\u001b[0m grid_search_results \u001b[38;5;241m=\u001b[39m \u001b[43mgrid_search_hyperparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moriginal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size_list\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 205\u001b[0m, in \u001b[0;36mgrid_search_hyperparameters\u001b[0;34m(base_dataset_path, model_name, loss_function_list, learning_rate_list, batch_size_list)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_size_list:\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n\u001b[0;32m--> 205\u001b[0m             hyperparameter_performance_results[(loss_function, learning_rate, batch_size)] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m             i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hyperparameter_performance_results\n",
      "Cell \u001b[0;32mIn[24], line 122\u001b[0m, in \u001b[0;36mtrain_and_evaluate_model\u001b[0;34m(base_dataset_path, model_name, loss_function, learning_rate, batch_size)\u001b[0m\n\u001b[1;32m    120\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()                                                  \u001b[38;5;66;03m# zero-ing out / restarting the gradients ... (BUT THE MODEL WEIGHTS ARE NEVER ZERO-ED!)\u001b[39;00m\n\u001b[1;32m    121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)                                                \u001b[38;5;66;03m# running all the input data through to get some predictions\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m                                      \u001b[38;5;66;03m# calculating loss wrt the loss function, comparing the predictions with the actuals\u001b[39;00m\n\u001b[1;32m    123\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()                                                        \u001b[38;5;66;03m# performs backprop\u001b[39;00m\n\u001b[1;32m    124\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()                                                       \u001b[38;5;66;03m# updates the model parameters using what was found during backprop\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_proj_mac/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_proj_mac/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_proj_mac/lib/python3.10/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_proj_mac/lib/python3.10/site-packages/torch/nn/functional.py:3328\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3326\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3328\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/miniconda3/envs/cv_proj_mac/lib/python3.10/site-packages/torch/functional.py:73\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (11) must match the size of tensor b (32) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "base_dataset_path = os.path.join('SVHN-dataset', 'cropped-digits')\n",
    "train_data_path = os.path.join(base_dataset_path, 'train.mat')\n",
    "test_data_path = os.path.join(base_dataset_path, 'test.mat')\n",
    "\n",
    "\n",
    "# Defining the Hyperparameters Whose Combination I'm Optimizing\n",
    "loss_function_list = ['cross-entropy', 'mean-squared', 'multi-margin']\n",
    "learning_rate_list = [0, 0.001, 0.1, 1]\n",
    "batch_size_list = [32, 64, 128, 256]\n",
    "\n",
    "\n",
    "grid_search_results = grid_search_hyperparameters(base_dataset_path, 'original', loss_function_list, learning_rate_list, batch_size_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17.333491776896675, 18.134790732869753, 20.082974800245857, 72.1759901046753)\n"
     ]
    }
   ],
   "source": [
    "print(grid_search_results[('cross-entropy', 0.01, 64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 81.30 %\n",
      "Test Loss: 0.6746\n"
     ]
    }
   ],
   "source": [
    "def apply_model_to_data(model_path, data_path, model_name, batch_size):\n",
    "\n",
    "    # Loading in the Model ......................................................................\n",
    "    if model_name == 'original':\n",
    "        model = CNN() \n",
    "    elif model_name == 'vgg16':\n",
    "        model = models.vgg16()\n",
    "        model.classifier[6] = nn.Linear(4096, 11)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))                                      # loading in the weights of the model!\n",
    "    model.eval()                                                                       # setting the model to evaluation mode\n",
    "\n",
    "    dataset = raw_data_to_tensor_dataset(data_path, batch_size)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "    # 3. Define Loss Function (Consistent with Training)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 4. Evaluate on Test Data\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predictions.append(predicted)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    test_loss /= total\n",
    "    accuracy = 100 * correct / total\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    print(f\"Accuracy on the test set: {accuracy:.2f} %\")\n",
    "    return test_loss, all_predictions, all_labels\n",
    "\n",
    "\n",
    "\n",
    "base_dataset_path = os.path.join('SVHN-dataset', 'cropped-digits')\n",
    "train_data_path = os.path.join(base_dataset_path, 'train.mat')\n",
    "test_data_path = os.path.join(base_dataset_path, 'test.mat')\n",
    "\n",
    "model_path = 'current-models/original/testing456.pth'\n",
    "# model_path = 'current-models/original/cross-entropy-64-0.01.pth'\n",
    "model_name = 'original'\n",
    "batch_size = 64\n",
    "\n",
    "test_loss, predictions, labels = apply_model_to_data(model_path, test_data_path, model_name, batch_size)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ('mean-squared', 0.01, 64)\n",
    "# grid_search_results[('mean-squared', 0.01, 64)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_proj_mac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
